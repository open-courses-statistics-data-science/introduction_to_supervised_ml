---
title: "Linear regression"
output: html_notebook
---

In this exercise, we are going to investigate linear regression: how to formulate the model, how to fit these models to data using gradient descent and we'll also start to investigate overfitting.

1. Suppose a linear regression model of the form:

\begin{equation}
y_i \stackrel{i.i.d.}{\sim} \mathcal{N}(\alpha + \beta x_i, \sigma)
\end{equation}

and assume you have access to $(y_i,x_i)_{i=1}^{K}$. Show that the maximum likelihood estimators of the parameters are the same as those minimising the mean squared loss function:

\begin{equation}
L = \frac{1}{K} \sum_{i=1}^{K} (y_i - (\alpha + \beta x_i))^2
\end{equation}

The likelihood is:

\begin{equation}
\mathcal{L} = \prod_{i=1}^{K} \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-(y_i - (\alpha + \beta x_i))^2/2\sigma^2)
\end{equation}

Writing down the log-likelihood, we have:

\begin{equation}
\mathcal{l} = - \frac{K}{2}\log 2\pi - \frac{K}{2}\log \sigma^2 - \frac{1}{2\sigma^2} \sum_{i=1}^{K} (y_i - (\alpha + \beta x_i))^2
\end{equation}

We now note that the squared loss term in the log-likelihood is of the same form as that in the mean squared loss function, so the parameter estimators are the same in both cases.


2. Generate simulated regression data via:

\begin{equation}
y_i \stackrel{i.i.d.}{\sim} \mathcal{N}(\alpha + \beta x_i, \sigma)
\end{equation}

where $\alpha=1$, $\beta=1$, $\sigma=1$ and $x_i \stackrel{i.i.d.}{\sim}\mathcal{N}(0, 4)$ for $i =1,...,100$.

```{r}
rm(list=ls())
library(tidyverse)
library(reshape2)
library(e1071)

alpha <- 1
beta <- 1
sigma <- 1
n <- 100
x <- rnorm(n, 0, 4)
y <- alpha + beta * x + rnorm(n, 0, sigma)
plot(x, y)
```

3. Create a function which implements one epoch of gradient descent and updates estimates of both $\alpha$ and $\beta$
```{r}
update_parameters <- function(alpha, beta, y, x, eta) {
  K <- length(y)
  dl_da <- -(2 / K) * sum(y - (alpha + beta * x))
  dl_db <- -(2 / K) * sum(x * (y - (alpha + beta * x)))
  
  alpha <- alpha - eta * dl_da
  beta <- beta - eta * dl_db
  return(c(alpha, beta))
}
```

4. Update parameters across many epochs and visualise their parameter estimates during training
```{r}
f_train_model <- function(n_epochs, eta, y, x, inits=c(0, 0)) {

  m_params <- matrix(nrow = n_epochs, ncol = 2)
  m_params[1, ] <- inits
  for(i in 2:n_epochs) {
    m_params[i, ] <- update_parameters(m_params[(i - 1), 1], m_params[(i - 1), 2], y, x, eta)
  }
  m_params <- as.data.frame(m_params)
  colnames(m_params) <- c("alpha", "beta")
  m_params <- m_params %>% 
    mutate(epoch=seq_along(alpha))
  return(m_params)
}
m_params <- f_train_model(2000, 0.01, y, x)
m_params %>% 
  melt(id.vars="epoch") %>% 
  ggplot(aes(x=epoch, y=value, colour=as.factor(variable))) +
  geom_line()
```

5. Draw the loss surface and use it to visualise the path of parameter estimates during training
```{r}
msl <- function(y, x, alpha, beta) {
  K <- length(y)
  return(1/K * sum((y-(alpha + beta * x))^2))
}

params <- expand_grid(alpha=seq(-2, 5, 0.1), beta=seq(-2, 5, 0.1))
z <- map2_dbl(params$alpha, params$beta, ~msl(y, x, .x, .y))
params <- params %>% 
  mutate(loss=z)

m_params <- m_params[1:100, ] %>% 
  mutate(type="training")

df1 <- params %>% 
  mutate(type="loss") %>% 
  bind_rows(m_params)

ggplot(df1 %>% filter(type=="loss"), aes(x=alpha, y=beta)) +
  geom_contour(aes(z=log(loss)), bins = 10) +
  geom_point(data=df1 %>% filter(type=="training")) +
  geom_line(data=df1 %>% filter(type=="training"))
```

6. How does choice of $\eta$ affect the rate of training?
```{r}
# Low eta
f_train_model(2000, 0.00001, y, x) %>% 
  melt(id.vars="epoch") %>% 
  ggplot(aes(x=epoch, y=value, colour=as.factor(variable))) +
  geom_line()

# High eta
f_train_model(2000, 0.05, y, x) %>% 
  melt(id.vars="epoch") %>% 
  ggplot(aes(x=epoch, y=value, colour=as.factor(variable))) +
  geom_line()
```

The higher $\eta$ is, the faster the rate of training.

7. Why does a high value of $\eta$ cause issues for training?

```{r}
m_params <- f_train_model(100, 0.1, y, x)

m_params <- m_params[1:4, ] %>% 
  mutate(type="training")

df1 <- params %>% 
  mutate(type="loss") %>% 
  bind_rows(m_params)

ggplot(df1 %>% filter(type=="loss"), aes(x=alpha, y=beta)) +
  geom_contour(aes(z=log(loss)), bins = 10) +
  geom_point(data=df1 %>% filter(type=="training")) +
  geom_line(data=df1 %>% filter(type=="training"))
```

If too high a value is chosen, the steps taken in a given direction overshoot the curvature, resulting in an increase in the loss. If these steps are repeatedly taken, the loss divergences.

8. For a reasonable value of $\eta$, visualise the mean squared loss over time
```{r}
m_params <- f_train_model(500, 0.01, y, x)

losses <- map_dbl(seq_along(m_params$alpha), ~msl(y, x, m_params$alpha[.], m_params$beta[.]))
m_params <- m_params %>% 
  mutate(loss=losses)
m_params %>% 
  ggplot(aes(x=epoch, y=loss)) +
  geom_line()
```

9. Overlay your estimated regression line on the data
```{r}
x_sim <- seq(-12, 12, 0.1)
y_sim <- m_params$alpha[nrow(m_params)] + m_params$beta[nrow(m_params)] * x_sim
df <- tibble(x, y, type="actual") %>% 
  bind_rows(tibble(x=x_sim, y=y_sim, type="simulated"))
ggplot(df %>% filter(type=="actual"), aes(x=x, y=y)) +
  geom_point() +
  geom_line(data=df %>% filter(type=="simulated"))

```

10. Generate similar data except with $\beta=0.02$ and $x\sim\mathcal{N}(0, 400)$. How is training impacted for this model? 
```{r}
alpha <- 1
beta <- 0.02
sigma <- 10
n <- 100
x <- rnorm(n, 0, 400)
y <- alpha + beta * x + rnorm(n, 0, sigma)
plot(x, y)

params <- expand_grid(alpha=seq(-2, 5, 0.1), beta=seq(-0.05, 0.05, 0.005))
z <- map2_dbl(params$alpha, params$beta, ~msl(y, x, .x, .y))
params <- params %>% 
  mutate(loss=z)

m_params <- f_train_model(1000, 0.0000001, y, x) %>% 
  mutate(type="training")

df1 <- params %>% 
  mutate(type="loss") %>% 
  bind_rows(m_params)

ggplot(df1 %>% filter(type=="loss"), aes(x=alpha, y=beta)) +
  geom_contour(aes(z=log(loss)), bins = 10) +
  geom_point(data=df1 %>% filter(type=="training")) +
  geom_line(data=df1 %>% filter(type=="training"))
```

Training is very slow for $\alpha$ due to the different parameter scales.

11. Standardise both the $y$ and $x$ variables and return training. How is the rate of convergence now?
```{r}
y_tilde <- scale(y)[, 1]
x_tilde <- scale(x)[, 1]

params <- expand_grid(alpha=seq(-2, 5, 0.1), beta=seq(-1, 2, 0.01))
z <- map2_dbl(params$alpha, params$beta, ~msl(y_tilde, x_tilde, .x, .y))
params <- params %>% 
  mutate(loss=z)

m_params <- f_train_model(1000, 0.01, y_tilde, x_tilde, inits = c(-1, -1)) %>% 
  mutate(type="training")

df1 <- params %>% 
  mutate(type="loss") %>% 
  bind_rows(m_params)

ggplot(df1 %>% filter(type=="loss"), aes(x=alpha, y=beta)) +
  geom_contour(aes(z=log(loss)), bins = 10) +
  geom_point(data=df1 %>% filter(type=="training")) +
  geom_line(data=df1 %>% filter(type=="training"))
```

Training now much faster: in summary, gradient descent much more efficient if data are on similar scales. So, feature scaling really important.

12. Generate regression data via:

\begin{equation}
y_i \stackrel{i.i.d.}{\sim} \mathcal{N}(\alpha + \beta x_i, \sigma)
\end{equation}

where $\alpha=1$, $\beta=1$, $\sigma=5$ and $x_i \stackrel{i.i.d.}{\sim}\mathcal{N}(0, 4)$ for $i =1,...,30$.

Use gradient descent to obtain parameter estimates for a quadratic regression and graph the fitted line versus the data.

```{r}
alpha <- 1
beta <- 1
sigma <- 5
n <- 30
x <- rnorm(n, 0, 4)
y <- alpha + beta * x + rnorm(n, 0, sigma)

update_parameters_quad <- function(alpha, beta, gamma, y, x, eta) {
  K <- length(y)
  dl_da <- -(2 / K) * sum(y - (alpha + beta * x + gamma * x^2))
  dl_db <- -(2 / K) * sum(x * (y - (alpha + beta * x + gamma * x^2)))
  dl_dg <- -(2 / K) * sum(x^2 * (y - (alpha + beta * x + gamma * x^2)))
  
  alpha <- alpha - eta * dl_da
  beta <- beta - eta * dl_db
  gamma <- gamma - eta * dl_dg
  return(c(alpha, beta, gamma))
}

n_epochs <- 5000
m_params <- matrix(nrow = n_epochs, ncol = 3)
m_params[1, ] <- c(0, 0, 0)
for(i in 2:n_epochs) {
  m_params[i, ] <- update_parameters_quad(m_params[(i - 1), 1], m_params[(i - 1), 2],
                                          m_params[(i - 1), 3], y, x, 0.001)
}
m_params <- as.data.frame(m_params)
colnames(m_params) <- c("alpha", "beta", "gamma")
m_params <- m_params %>% 
  mutate(epoch=seq_along(alpha))
m_params %>% 
  melt(id.vars="epoch") %>% 
  ggplot(aes(x=epoch, y=value, colour=as.factor(variable))) +
  geom_line()

x_sim <- seq(-12, 12, 0.1)
y_sim <- m_params$alpha[nrow(m_params)] + m_params$beta[nrow(m_params)] * x_sim + m_params$gamma[nrow(m_params)] * x_sim^2

f_quadratic_regression <- function(x, alpha, beta, gamma) {
  return(alpha + beta * x + gamma * x^2)
}
a <- m_params$alpha[nrow(m_params)]
b <- m_params$beta[nrow(m_params)]
c <- m_params$gamma[nrow(m_params)]
f_quad <- function(x) f_quadratic_regression(x, a, b, c)

df <- tibble(x, y, type="actual") %>% 
  bind_rows(tibble(x=x_sim, y=y_sim, type="simulated"))
ggplot(df %>% filter(type=="actual"), aes(x=x, y=y)) +
  geom_point() +
  geom_line(data=df %>% filter(type=="simulated"))
```

13. Fit the same data as generated in the previous question using linear regression. Compare the fit of the quadratic and linear models on new data generated from the same data generating process except:

$x_i \stackrel{i.i.d.}{\sim}\mathcal{N}(5, 4)$ for $i =1,...,30$

Which has a better root mean squared accuracy on the new data? Why?

```{r}
m_params <- f_train_model(2000, 0.01, y, x)

f_linear_regression <- function(x, alpha, beta) {
  return(alpha + beta * x)
}

f_linear <- function(x) f_linear_regression(x,m_params$alpha[nrow(m_params)],
                                             m_params$beta[nrow(m_params)])

x1 <- rnorm(n, 5, 4)
y1 <- alpha + beta * x1 + rnorm(n, 0, sigma)

# predict y and calculate RMSEs
y1_linear <- map_dbl(x1, f_linear)
y1_quad <- map_dbl(x1, f_quad)

sqrt(mean(y1_linear - y1)^2)
sqrt(mean(y1_quad - y1)^2)
```

The linear model generalises better. The quadratic model has overfitted the training data, so doesn't extrapolate well to new data regimes.
