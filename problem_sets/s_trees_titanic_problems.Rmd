---
title: "Building your own decision tree classifier: titanic data"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

1. Load the Titanic dataset.

2. In this question, we are going to start to build a regression tree classifier, where we try to determine the factors that influence whether a passenger survived.

Specifically, we are going to investigate how the passenger class, age and sex influenced survival.

For passenger class, we are going to use dummy variables to represent the three possible states: binary variables which take on the value 0 if not true and 1 if true.

Create dummy variables for classes 1 and 2. This implicitly means that the 3rd class will be the base case that we compare to.

3. Create a dummy variable equal to 1 if the passenger was female.

4. For age, we are going to split the data up into three segments: (i) those aged 16 or less; (ii) those between 16 and 60; (iii) and those over 60. Create dummy variables for categories (i) and (iii).

5. Remove all of the columns apart from the survival column and the sets of dummy variables you have created.

6. Remove any data points with missing observations.

7. The entropy of a binary outcome variable, $X_i$, is given by:

$$ H = -p \log p - (1 - p) \log (1 - p),$$

where $p=\mathbb{P}(X=1)$.

Write a function which can calculate the entropy of a binary vector. Use it to calculate the entropy of the survival variable in the full dataset.

8. We now start to build a decision tree classifier. To do so, we are going to choose one of the five variables to split on based on the reduction in entropy this provides. To do so, we calculate the conditional entropy, $H(X|V)$, where $V$ is a particular variable we have split on. At each step, we will choose the variable to split on so that it results in the greatest reduction in entropy.

The conditional entropy for a binary variable, $V$, is given by:

$$H(X|V) = S(V=1)/S(\phi) \times H(S(V=1)) + S(V=0)/S(\phi) \times H(S(V=0)),$$

where $S(V=v)$ is the set of members of the random variable $X$ corresponding to $V=v$. For example, consider:

```{r}
df_example <- tribble(
  ~X, ~V,
  0, 1,
  1, 1,
  0, 1,
  1, 0,
  0, 0,
  1, 1
)

df_example
```
then $S(V=1)$ is the $X$ column from the subsetted data frame:

```{r}
df_example %>% 
  filter(V==1)
```


Write a function to calculate the conditional entropy of splitting on a particular variable for your dataset.

9. Use the function you created in the previous question to determine the reduction in entropy from splitting on each of the five possible variables. Which column yields the greatest reduction in entropy?

10. Explain intuitively why splitting on that variable resulted in the greatest reduction in entropy?

11. Create a decision tree classifier using the variable you have identified. The classifier outputs a classification probability:

$$\mathbb{P}(X=1|V=v) = \frac{1}{S(V=v)} \sum_{i\in S(V=v)} X_i,$$

where $X$ denotes the survival variable and $V$ denotes the variable you split on. The above just means your outputted probability of survival is the corresponding fraction surviving in the subset corresponding to that particular value of the variable $V$.

11. Create a decision tree classifier with depth 2 (i.e. it splits on two variables), where in each step it chooses which variable to split on based on the greatest reduction in entropy.

12. Use your classifier to output the probabilities of survival for each (type of) individual in your dataset. Which groups have the highest survival probabilities and the lowest survival probabilities?
